groups:
- name: cortex-nova-alerts
  rules:
  - alert: CortexNovaSchedulingDown
    expr: |
      up{pod=~"cortex-nova-scheduling-.*"} != 1 or
      absent(up{pod=~"cortex-nova-scheduling-.*"})
    for: 5m
    labels:
      context: liveness
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
      playbook: docs/support/playbook/cortex/down
    annotations:
      summary: "Cortex Scheduling for Nova is down"
      description: >
        The Cortex scheduling service is down. Scheduling requests from Nova will
        not be served. This is no immediate problem, since Nova will continue
        placing new VMs. However, the placement will be less desirable.

  - alert: CortexNovaKnowledgeDown
    expr: |
      up{pod=~"cortex-nova-knowledge-.*"} != 1 or
      absent(up{pod=~"cortex-nova-knowledge-.*"})
    for: 5m
    labels:
      context: liveness
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
      playbook: docs/support/playbook/cortex/down
    annotations:
      summary: "Cortex Knowledge for Nova is down"
      description: >
        The Cortex Knowledge service is down. This is no immediate problem,
        since cortex is still able to process requests,
        but the quality of the responses may be affected.

  - alert: CortexNovaDeschedulerPipelineErroring
    expr: delta(cortex_descheduler_pipeline_vm_descheduling_duration_seconds_count{component="nova-scheduling", error="true"}[2m]) > 0
    for: 5m
    labels:
      context: descheduler
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
    annotations:
      summary: "Descheduler pipeline is erroring."
      description: >
        The Cortex descheduler pipeline is encountering errors during its execution.
        This may indicate issues with the descheduling logic or the underlying infrastructure.
        It is recommended to investigate the descheduler logs and the state of the VMs being processed.

  - alert: CortexNovaHttpRequest400sTooHigh
    expr: rate(cortex_scheduler_api_request_duration_seconds_count{service="cortex-nova-metrics", status=~"4.+"}[5m]) > 0.1
    for: 5m
    labels:
      context: api
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
    annotations:
      summary: "Nova Scheduler HTTP request 400 errors too high"
      description: >
        Nova Scheduler is responding to placement requests with HTTP 4xx
        errors. This is expected when the scheduling request cannot be served
        by Cortex. However, it could also indicate that the request format has
        changed and Cortex is unable to parse it.

  - alert: CortexNovaSchedulingHttpRequest500sTooHigh
    expr: rate(cortex_scheduler_api_request_duration_seconds_count{service="cortex-nova-metrics", status=~"5.+" }[5m]) > 0.1
    for: 5m
    labels:
      context: api
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
    annotations:
      summary: "Nova Scheduler HTTP request 500 errors too high"
      description: >
        Nova Scheduler is responding to placement requests with HTTP 5xx errors.
        This is not expected and indicates that Cortex is having some internal problem.
        Nova will continue to place new VMs, but the placement will be less desirable.
        Thus, no immediate action is needed.

  - alert: CortexNovaHighMemoryUsage
    expr: process_resident_memory_bytes{service="cortex-nova-metrics"} > 6000 * 1024 * 1024
    for: 5m
    labels:
      context: memory
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
    annotations:
      summary: "`{{$labels.component}}` uses too much memory"
      description: >
        `{{$labels.component}}` should not be using more than 6000 MiB of memory. Usually it
        should use much less, so there may be a memory leak or other changes
        that are causing the memory usage to increase significantly.

  - alert: CortexNovaHighCPUUsage
    expr: rate(process_cpu_seconds_total{service="cortex-nova-metrics"}[1m]) > 0.5
    for: 5m
    labels:
      context: cpu
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
    annotations:
      summary: "`{{$labels.component}}` uses too much CPU"
      description: >
        `{{$labels.component}}` should not be using more than 50% of a single CPU core. Usually
        it should use much less, so there may be a CPU leak or other changes
        that are causing the CPU usage to increase significantly.

  - alert: CortexNovaTooManyDBConnectionAttempts
    expr: rate(cortex_db_connection_attempts_total{service="cortex-nova-metrics"}[5m]) > 0.1
    for: 5m
    labels:
      context: db
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
    annotations:
      summary: "`{{$labels.component}}` is trying to connect to the database too often"
      description: >
        `{{$labels.component}}` is trying to connect to the database too often. This may happen
        when the database is down or the connection parameters are misconfigured.

  - alert: CortexNovaSyncNotSuccessful
    expr: cortex_sync_request_processed_total{service="cortex-nova-metrics"} - cortex_sync_request_duration_seconds_count{service="cortex-nova-metrics"} > 0
    for: 5m
    labels:
      context: syncstatus
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
    annotations:
      summary: "`{{$labels.component}}` Sync not successful"
      description: >
        `{{$labels.component}}` experienced an issue syncing data from the datasource `{{$labels.datasource}}`. This may
        happen when the datasource (OpenStack, Prometheus, etc.) is down or
        the sync module is misconfigured. No immediate action is needed, since
        the sync module will retry the sync operation and the currently synced
        data will be kept. However, when this problem persists for a longer
        time the service will have a less recent view of the datacenter.

  - alert: CortexNovaSyncObjectsDroppedToZero
    expr: cortex_sync_objects{service="cortex-nova-metrics", datasource!="openstack_migrations"} == 0
    for: 60m
    labels:
      context: syncobjects
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
    annotations:
      summary: "`{{$labels.component}}` is not syncing any new data from `{{$labels.datasource}}`"
      description: >
        `{{$labels.component}}` is not syncing any objects from the datasource `{{$labels.datasource}}`. This may happen
        when the datasource (OpenStack, Prometheus, etc.) is down or the sync
        module is misconfigured. No immediate action is needed, since the sync
        module will retry the sync operation and the currently synced data will
        be kept. However, when this problem persists for a longer time the
        service will have a less recent view of the datacenter.

  - alert: CortexNovaDatasourceUnready
    expr: cortex_datasource_state{operator="cortex-nova",state=~"waiting|error|unknown"} != 0
    for: 60m
    labels:
      context: datasources
      dashboard: cortex/cortex
      service: cortex
      severity: warning
      support_group: workload-management
    annotations:
      summary: "Datasource `{{$labels.datasource}}` is in `{{$labels.state}}` state"
      description: >
        This may indicate issues with the datasource
        connectivity or configuration. It is recommended to investigate the
        datasource status and logs for more details.
